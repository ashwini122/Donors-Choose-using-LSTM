{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary lib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, BatchNormalization,concatenate,Flatten,Embedding,Dense,Dropout,MaxPooling2D,CuDNNLSTM,SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras import Model,Input\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "import keras.backend as k\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>...</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>presence_of_the_numerical_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>mrs</td>\n",
       "      <td>in</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl_literacy</td>\n",
       "      <td>educational support english learners home</td>\n",
       "      <td>...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>mr</td>\n",
       "      <td>fl</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>history_civics_health_sports</td>\n",
       "      <td>civics_government_teamsports</td>\n",
       "      <td>wanted projector hungry learners</td>\n",
       "      <td>...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>p182444</td>\n",
       "      <td>3465aaf82da834c0582ebd0ef8040ca0</td>\n",
       "      <td>ms</td>\n",
       "      <td>az</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>health_sports</td>\n",
       "      <td>health_wellness_teamsports</td>\n",
       "      <td>soccer equipment awesome middle school students</td>\n",
       "      <td>...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0           0  p253737  c90749f5d961ff158d4b4d1e7dc665fc            mrs   \n",
       "1           1  p258326  897464ce9ddc600bced1151f324dd63a             mr   \n",
       "2           2  p182444  3465aaf82da834c0582ebd0ef8040ca0             ms   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           in        2016-12-05 13:43:57          grades_prek_2   \n",
       "1           fl        2016-10-25 09:22:10             grades_6_8   \n",
       "2           az        2016-08-31 12:03:56             grades_6_8   \n",
       "\n",
       "     project_subject_categories project_subject_subcategories  \\\n",
       "0             literacy_language                  esl_literacy   \n",
       "1  history_civics_health_sports  civics_government_teamsports   \n",
       "2                 health_sports    health_wellness_teamsports   \n",
       "\n",
       "                                     project_title  \\\n",
       "0        educational support english learners home   \n",
       "1                 wanted projector hungry learners   \n",
       "2  soccer equipment awesome middle school students   \n",
       "\n",
       "                 ...                 \\\n",
       "0                ...                  \n",
       "1                ...                  \n",
       "2                ...                  \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "2  The students on the campus come to school know...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "2             NaN  My students need shine guards, athletic socks,...   \n",
       "\n",
       "  teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            0                    0   \n",
       "1                                            7                    1   \n",
       "2                                            1                    0   \n",
       "\n",
       "                                               essay   price  quantity  \\\n",
       "0  students english learners working english seco...  154.60        23   \n",
       "1  students arrive school eager learn polite gene...  299.00         1   \n",
       "2  true champions not always ones win guts mia ha...  516.85        22   \n",
       "\n",
       "   presence_of_the_numerical_digits  \n",
       "0                                 0  \n",
       "1                                 0  \n",
       "2                                 0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "project_data = pd.read_csv('processed_train_data.csv')\n",
    "project_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining teacher number of previously posted projects, presence of the numberical digits price and quantity into a single feature\n",
    "project_data.drop(['Unnamed: 0'], axis =1 , inplace = True)\n",
    "class_label = project_data['project_is_approved']\n",
    "project_data['remaining_input'] = project_data['teacher_number_of_previously_posted_projects']  +\\\n",
    "                                    project_data['presence_of_the_numerical_digits']  + \\\n",
    "                                    project_data['price'] + project_data['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "train,test,y_train,y_test = train_test_split(project_data, class_label , stratify = class_label, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,cv,y_train,y_cv = train_test_split(train,y_train,stratify = y_train,train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Train dataset:  61178\n",
      "Shape of the Test dataset:  32775\n",
      "Shape of the CV Dataset: 15295\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Train dataset: \", train.shape[0])\n",
    "print(\"Shape of the Test dataset: \", test.shape[0])\n",
    "print(\"Shape of the CV Dataset:\", cv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the class labels to one hot encoding for keras model evaluation\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_cv = to_categorical(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the pre trained word vectors file\n",
    "dbfile = open('glove_vectors.pickle', 'rb')      \n",
    "db = pickle.load(dbfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix of word as rows and columns as 50 dmin vectors of words\n",
    "def embedding_mat(word_index,embedding_dim = 300):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector =db.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the Top features based on IDF values for the Text part of the project_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'IDF score')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tfidf vectorization of text data\n",
    "tfidf = TfidfVectorizer()\n",
    "data_text = tfidf.fit_transform(train['essay'])\n",
    "plt.boxplot(tfidf.idf_)\n",
    "plt.ylabel(\"IDF score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25 percentile of idf score is : [9.31350907]\n",
      "The 75 percentile of idf score is : [11.32841209]\n"
     ]
    }
   ],
   "source": [
    "print(\"The 25 percentile of idf score is :\", np.percentile(tfidf.idf_,[25]))\n",
    "print(\"The 75 percentile of idf score is :\",np.percentile(tfidf.idf_,[75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting the features based on idf score. All the features with idf score between 7.21480808 and 8.82424599 are selected for the model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idf = zip(tfidf.get_feature_names(),tfidf.idf_)\n",
    "\n",
    "feature_name = []\n",
    "for x,y in feature_idf:\n",
    "    \n",
    "    if y >=9.31350907 and 11.32841209 :\n",
    "        feature_name.append(x)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only those words which have idf values between 25th percentile to 75th percentile\n",
    "def few_text(df):\n",
    "    processed_text = []\n",
    "    for text in df:\n",
    "        sent = \" \"\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word in feature_name:\n",
    "                sent = ' ' + word\n",
    "            else:\n",
    "                pass\n",
    "        processed_text.append(sent)\n",
    "    return processed_text\n",
    "\n",
    "train['processed_essay'] = few_text(train['essay'])\n",
    "test['processed_essay'] = few_text(test['essay'])\n",
    "cv['processed_essay'] = few_text(cv['essay'])\n",
    "\n",
    "train.to_csv(\"model-train.csv\")\n",
    "test.to_csv(\"model-test.csv\")\n",
    "cv.to_csv(\"model-cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"model-train.csv\")\n",
    "test = pd.read_csv(\"model-test.csv\")\n",
    "cv = pd.read_csv(\"model-cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['total_txt'] = train['project_title'] + ' ' + train['essay'] + ' ' + train['project_resource_summary']\n",
    "test['total_txt'] = test['project_title'] + ' ' + test['essay'] + ' ' + test['project_resource_summary']\n",
    "cv['total_txt'] = cv['project_title'] + ' ' + cv['essay'] + ' ' + cv['project_resource_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['project_is_approved']\n",
    "y_test = test['project_is_approved']\n",
    "y_cv = cv['project_is_approved']\n",
    "\n",
    "# converting the class labels to one hot encoding for keras model evaluation\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_cv = to_categorical(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ranking(train,test,cv):\n",
    "    col_names = train.columns\n",
    "    features = []\n",
    "    #performing train test split\n",
    "    \n",
    "    for col in col_names[:6]:\n",
    "        print(col)\n",
    "        bag_of_words = CountVectorizer(lowercase= False)\n",
    "        bow_words = bag_of_words.fit_transform(train[col])\n",
    "        print(bow_words.shape)\n",
    "        \n",
    "        #Lets now store the document term matrix in a dictionary.\n",
    "        freqs = bow_words.sum(axis=0).A1\n",
    "        index = freqs.argsort()\n",
    "        words = bag_of_words.get_feature_names()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Assigning Rank to each word based on its freq of occurance. Word with highest freq is assigned rank 1 \n",
    "        word_rank = dict()\n",
    "        rank = 1\n",
    "        for i in index[::-1]:\n",
    "            k = words[i]\n",
    "            word_rank[k] = rank\n",
    "            rank+=1\n",
    "        features.append(word_rank)\n",
    "\n",
    "        #Every word in each review is replaced by its rank\n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in train[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        train[col] = rank\n",
    "        \n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in test[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        test[col] = rank\n",
    "        \n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in cv[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        cv[col] = rank\n",
    "    return train,test,cv,features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['id','teacher_id','project_submitted_datetime','project_title','project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4','project_resource_summary',\n",
    "       'teacher_number_of_previously_posted_projects', 'project_is_approved','price', 'quantity',\n",
    "        'presence_of_the_numerical_digits','essay']\n",
    "\n",
    "train.drop(labels=col,axis =1, inplace=True)\n",
    "test.drop(labels=col,axis =1, inplace=True)\n",
    "cv.drop(labels=col,axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['teacher_prefix', 'school_state', 'project_grade_category',\n",
    "       'project_subject_categories', 'project_subject_subcategories','total_txt',\n",
    "       'remaining_input']\n",
    "train = train[col]\n",
    "test = test[col]\n",
    "test = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace(to_replace=np.NaN, value= str('nan'),inplace=True)\n",
    "test.replace(to_replace=np.NaN, value= str('nan'),inplace=True)\n",
    "cv.replace(to_replace=np.NaN, value= str('nan'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher_prefix\n",
      "(61178, 5)\n",
      "school_state\n",
      "(61178, 51)\n",
      "project_grade_category\n",
      "(61178, 4)\n",
      "project_subject_categories\n",
      "(61178, 50)\n",
      "project_subject_subcategories\n",
      "(61178, 380)\n",
      "total_txt\n",
      "(61178, 56912)\n"
     ]
    }
   ],
   "source": [
    "train,test,cv,feature_names =  word_ranking(train,test,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the Test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 250)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0  180   19  132    1  368    2   28   23  111\n",
      "  151   11   99   68   52 1638    2   73   30 1816    1   78    4  580\n",
      " 1373 1808  427    1   45   17    7 1285  432  294    5   17  123  103\n",
      "   72 2069  323 2305    1   56  461    1   25  260  863  572 2973 4302\n",
      "    5  247    4  863  287    1  119  645  811   27  714   76  119  697\n",
      "  579   24  486   74  119  229 2973 4302  287    1  533  497    4   51\n",
      " 1039 1995  541 4841 1282 2923   30  157   27 2594 1193 1145  325  210\n",
      "  215  166   20   63 3257  215   13   10    1    3   19  199  194   39\n",
      "  145    9   48   15  247   65  743 3934 8518  287   83    9  410 1270\n",
      "  794   77  802    1    9 2633  239   27  316   21    9  476]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_review_length = 250\n",
    "X_train = pad_sequences(train['total_txt'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test = pad_sequences(test['total_txt'], maxlen=max_review_length)\n",
    "X_cv = pad_sequences(cv['total_txt'], maxlen=max_review_length)\n",
    "print(X_train.shape)\n",
    "print(X_train[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the school state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32775, 1)\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_review_length = 1\n",
    "X_train_school_state = pad_sequences(train['school_state'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test_school_state = pad_sequences(test['school_state'], maxlen=max_review_length)\n",
    "X_cv_school_state = pad_sequences(cv['school_state'], maxlen=max_review_length)\n",
    "print(X_test_school_state.shape)\n",
    "print(X_test_school_state[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project_grade_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 1)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 1\n",
    "X_train_project_grade = pad_sequences(train['project_grade_category'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test_project_grade = pad_sequences(test['project_grade_category'], maxlen=max_review_length)\n",
    "X_cv_project_grade = pad_sequences(cv['project_grade_category'], maxlen=max_review_length)\n",
    "print(X_train_project_grade.shape)\n",
    "print(X_train_project_grade[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 1)\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 1\n",
    "X_train_project_cat = pad_sequences(train['project_subject_categories'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test_project_cat = pad_sequences(test['project_subject_categories'], maxlen=max_review_length)\n",
    "X_cv_project_cat = pad_sequences(cv['project_subject_categories'], maxlen=max_review_length)\n",
    "print(X_train_project_cat.shape)\n",
    "print(X_train_project_cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 1)\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 1\n",
    "X_train_project_subcat = pad_sequences(train['project_subject_subcategories'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test_project_subcat = pad_sequences(test['project_subject_subcategories'], maxlen=max_review_length)\n",
    "X_cv_project_subcat = pad_sequences(cv['project_subject_subcategories'], maxlen=max_review_length)\n",
    "print(X_train_project_subcat.shape)\n",
    "print(X_train_project_subcat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the teacher prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 1)\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 1\n",
    "X_train_teacher_prefix = pad_sequences(train['teacher_prefix'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test_teacher_prefix = pad_sequences(test['teacher_prefix'], maxlen=max_review_length)\n",
    "X_cv_teacher_prefix = pad_sequences(cv['teacher_prefix'], maxlen=max_review_length)\n",
    "print(X_train_teacher_prefix.shape)\n",
    "print(X_test_teacher_prefix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>total_txt</th>\n",
       "      <th>remaining_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1]</td>\n",
       "      <td>[31]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[758, 33, 863, 33, 6464, 33, 5526, 112, 286, 2...</td>\n",
       "      <td>147.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[36]</td>\n",
       "      <td>[1747, 1035, 365, 122, 1, 6037, 1041, 365, 122...</td>\n",
       "      <td>135.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[37]</td>\n",
       "      <td>[220]</td>\n",
       "      <td>[246, 282, 1085, 763, 79, 96, 144, 47, 135, 2,...</td>\n",
       "      <td>457.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1]</td>\n",
       "      <td>[20]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[6784, 1191, 1366, 2463, 344, 332, 52, 1, 198,...</td>\n",
       "      <td>57.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[126]</td>\n",
       "      <td>[11130, 1191, 58, 5811, 824, 1, 23, 96, 144, 3...</td>\n",
       "      <td>444.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  teacher_prefix school_state project_grade_category  \\\n",
       "0            [1]         [31]                    [1]   \n",
       "1            [1]         [12]                    [2]   \n",
       "2            [1]          [8]                    [3]   \n",
       "3            [1]         [20]                    [2]   \n",
       "4            [1]         [19]                    [2]   \n",
       "\n",
       "  project_subject_categories project_subject_subcategories  \\\n",
       "0                        [2]                           [5]   \n",
       "1                       [17]                          [36]   \n",
       "2                       [37]                         [220]   \n",
       "3                        [3]                           [2]   \n",
       "4                        [9]                         [126]   \n",
       "\n",
       "                                           total_txt  remaining_input  \n",
       "0  [758, 33, 863, 33, 6464, 33, 5526, 112, 286, 2...           147.89  \n",
       "1  [1747, 1035, 365, 122, 1, 6037, 1041, 365, 122...           135.39  \n",
       "2  [246, 282, 1085, 763, 79, 96, 144, 47, 135, 2,...           457.30  \n",
       "3  [6784, 1191, 1366, 2463, 344, 332, 52, 1, 198,...            57.99  \n",
       "4  [11130, 1191, 58, 5811, 824, 1, 23, 96, 144, 3...           444.04  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    " ### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC score\n",
    "def auc( y_true, y_pred ) :\n",
    "    score = tf.py_func( lambda y_true, y_pred : roc_auc_score( y_true, y_pred ,average='weighted').astype('float32'),\n",
    "                        [y_true, y_pred],\n",
    "                        'float32',\n",
    "                        stateful=True,\n",
    "                        name='sklearnAUC' )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 250, 300)     17073900    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 250, 300)     0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 250, 128)     220160      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 2)         104         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 2)         10          input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 2)         100         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 50)        19250       input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 5)         30          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 32000)        0           cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2)            0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 50)           0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 5)            0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           32          input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32077)        0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          4105984     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           8256        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           2080        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            66          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,430,228\n",
      "Trainable params: 4,356,200\n",
      "Non-trainable params: 17,074,028\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#input 1\n",
    "input1 = Input(shape=(250,))\n",
    "x1 = Embedding(input_dim=56913,output_dim= 300,weights=[embedding_mat(feature_names[5])],trainable=False)(input1)\n",
    "x1 = SpatialDropout1D(0.3)(x1)\n",
    "x1 = CuDNNLSTM(128,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "#input 2\n",
    "input2 = Input(shape=(1,))\n",
    "x2 = Embedding(input_dim= 52,output_dim= 2)(input2)\n",
    "#x2 = SpatialDropout1D(0.3)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "#input 3\n",
    "input3 = Input(shape=(1,))\n",
    "x3 = Embedding(input_dim= 5,output_dim= 2)(input3)\n",
    "#x3 = SpatialDropout1D(0.3)(x3)\n",
    "x3 = Flatten()(x3)\n",
    "\n",
    "#input 4\n",
    "input4 = Input(shape=(1,))\n",
    "x4 = Embedding(input_dim=50,output_dim= 2)(input4)\n",
    "#x4 = SpatialDropout1D(0.3)(x4)\n",
    "x4 = Flatten()(x4)\n",
    "\n",
    "#input 5\n",
    "input5 = Input(shape=(1,))\n",
    "x5 = Embedding(input_dim= 385,output_dim= 50)(input5)\n",
    "#x5 = SpatialDropout1D(0.3)(x5)\n",
    "x5 = Flatten()(x5)\n",
    "\n",
    "#input 6\n",
    "input6 = Input(shape=(1,))\n",
    "x6 = Embedding(input_dim= 6,output_dim= 5)(input6)\n",
    "#x6 = SpatialDropout1D(0.3)(x6)\n",
    "x6 = Flatten()(x6)\n",
    "\n",
    "#input 7\n",
    "input7 = Input(shape=(1,))\n",
    "x7 = Dense(16,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(input7)\n",
    "#x7 = Flatten()(x7)\n",
    "#merging all the inputs \n",
    "concat = concatenate([x1,x2,x3,x4,x5,x6,x7])\n",
    "#x = BatchNormalization()(concat)\n",
    "\n",
    "x = Dense(128,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    " \n",
    "# create model with seven inputs\n",
    "model = Model([input1,input2,input3,input4,input5,input6,input7], output)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0006,decay = 1e-4),metrics=[auc])\n",
    "#lrate = LearningRateScheduler(step_decay)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61178 samples, validate on 15295 samples\n",
      "Epoch 1/50\n",
      "61178/61178 [==============================] - 28s 461us/step - loss: 0.6874 - auc: 0.5077 - val_loss: 0.4759 - val_auc: 0.5832\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58320, saving model to weights_2.best.hdf5\n",
      "Epoch 2/50\n",
      "61178/61178 [==============================] - 27s 433us/step - loss: 0.5224 - auc: 0.5228 - val_loss: 0.4537 - val_auc: 0.6028\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58320 to 0.60285, saving model to weights_2.best.hdf5\n",
      "Epoch 3/50\n",
      "61178/61178 [==============================] - 27s 435us/step - loss: 0.4816 - auc: 0.5384 - val_loss: 0.4451 - val_auc: 0.6168\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.60285 to 0.61685, saving model to weights_2.best.hdf5\n",
      "Epoch 4/50\n",
      "61178/61178 [==============================] - 27s 436us/step - loss: 0.4636 - auc: 0.5489 - val_loss: 0.4436 - val_auc: 0.6200\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.61685 to 0.61998, saving model to weights_2.best.hdf5\n",
      "Epoch 5/50\n",
      "61178/61178 [==============================] - 27s 438us/step - loss: 0.4433 - auc: 0.6070 - val_loss: 0.4227 - val_auc: 0.6426\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.61998 to 0.64257, saving model to weights_2.best.hdf5\n",
      "Epoch 6/50\n",
      "61178/61178 [==============================] - 27s 439us/step - loss: 0.4491 - auc: 0.5560 - val_loss: 0.4381 - val_auc: 0.6410\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.64257\n",
      "Epoch 7/50\n",
      "61178/61178 [==============================] - 27s 438us/step - loss: 0.4275 - auc: 0.6455 - val_loss: 0.4048 - val_auc: 0.7076\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.64257 to 0.70761, saving model to weights_2.best.hdf5\n",
      "Epoch 8/50\n",
      "61178/61178 [==============================] - 27s 440us/step - loss: 0.4160 - auc: 0.6778 - val_loss: 0.3990 - val_auc: 0.7266\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.70761 to 0.72656, saving model to weights_2.best.hdf5\n",
      "Epoch 9/50\n",
      "61178/61178 [==============================] - 27s 440us/step - loss: 0.4094 - auc: 0.6928 - val_loss: 0.4074 - val_auc: 0.7390\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.72656 to 0.73905, saving model to weights_2.best.hdf5\n",
      "Epoch 10/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.4063 - auc: 0.7016 - val_loss: 0.3952 - val_auc: 0.7416\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.73905 to 0.74161, saving model to weights_2.best.hdf5\n",
      "Epoch 11/50\n",
      "61178/61178 [==============================] - 27s 441us/step - loss: 0.4016 - auc: 0.7098 - val_loss: 0.3879 - val_auc: 0.7484\n",
      "\n",
      "Epoch 00011: val_auc improved from 0.74161 to 0.74838, saving model to weights_2.best.hdf5\n",
      "Epoch 12/50\n",
      "61178/61178 [==============================] - 27s 440us/step - loss: 0.3980 - auc: 0.7157 - val_loss: 0.3886 - val_auc: 0.7427\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.74838\n",
      "Epoch 13/50\n",
      "61178/61178 [==============================] - 27s 441us/step - loss: 0.3955 - auc: 0.7208 - val_loss: 0.3842 - val_auc: 0.7527\n",
      "\n",
      "Epoch 00013: val_auc improved from 0.74838 to 0.75274, saving model to weights_2.best.hdf5\n",
      "Epoch 14/50\n",
      "61178/61178 [==============================] - 27s 441us/step - loss: 0.3927 - auc: 0.7260 - val_loss: 0.3839 - val_auc: 0.7579\n",
      "\n",
      "Epoch 00014: val_auc improved from 0.75274 to 0.75794, saving model to weights_2.best.hdf5\n",
      "Epoch 15/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3903 - auc: 0.7325 - val_loss: 0.3822 - val_auc: 0.7582\n",
      "\n",
      "Epoch 00015: val_auc improved from 0.75794 to 0.75818, saving model to weights_2.best.hdf5\n",
      "Epoch 16/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.3882 - auc: 0.7362 - val_loss: 0.3797 - val_auc: 0.7579\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.75818\n",
      "Epoch 17/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.3846 - auc: 0.7408 - val_loss: 0.3770 - val_auc: 0.7570\n",
      "\n",
      "Epoch 00017: val_auc did not improve from 0.75818\n",
      "Epoch 18/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.3821 - auc: 0.7476 - val_loss: 0.3755 - val_auc: 0.7598\n",
      "\n",
      "Epoch 00018: val_auc improved from 0.75818 to 0.75982, saving model to weights_2.best.hdf5\n",
      "Epoch 19/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.3797 - auc: 0.7504 - val_loss: 0.3751 - val_auc: 0.7643\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.75982 to 0.76434, saving model to weights_2.best.hdf5\n",
      "Epoch 20/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.3778 - auc: 0.7553 - val_loss: 0.3729 - val_auc: 0.7634\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.76434\n",
      "Epoch 21/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3750 - auc: 0.7588 - val_loss: 0.3743 - val_auc: 0.7642\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.76434\n",
      "Epoch 22/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3721 - auc: 0.7636 - val_loss: 0.3756 - val_auc: 0.7608\n",
      "\n",
      "Epoch 00022: val_auc did not improve from 0.76434\n",
      "Epoch 23/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3687 - auc: 0.7692 - val_loss: 0.3729 - val_auc: 0.7616\n",
      "\n",
      "Epoch 00023: val_auc did not improve from 0.76434\n",
      "Epoch 24/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3681 - auc: 0.7710 - val_loss: 0.3749 - val_auc: 0.7651\n",
      "\n",
      "Epoch 00024: val_auc improved from 0.76434 to 0.76513, saving model to weights_2.best.hdf5\n",
      "Epoch 25/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3646 - auc: 0.7765 - val_loss: 0.3706 - val_auc: 0.7664\n",
      "\n",
      "Epoch 00025: val_auc improved from 0.76513 to 0.76645, saving model to weights_2.best.hdf5\n",
      "Epoch 26/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3622 - auc: 0.7826 - val_loss: 0.3730 - val_auc: 0.7634\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.76645\n",
      "Epoch 27/50\n",
      "61178/61178 [==============================] - 27s 445us/step - loss: 0.3602 - auc: 0.7852 - val_loss: 0.3738 - val_auc: 0.7660\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.76645\n",
      "Epoch 28/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3592 - auc: 0.7858 - val_loss: 0.3743 - val_auc: 0.7651\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.76645\n",
      "Epoch 29/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3544 - auc: 0.7936 - val_loss: 0.3744 - val_auc: 0.7620\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.76645\n",
      "Epoch 30/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3517 - auc: 0.7988 - val_loss: 0.3760 - val_auc: 0.7585\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.76645\n",
      "Epoch 31/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3488 - auc: 0.8049 - val_loss: 0.3788 - val_auc: 0.7562\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.76645\n",
      "Epoch 32/50\n",
      "61178/61178 [==============================] - 27s 445us/step - loss: 0.3452 - auc: 0.8088 - val_loss: 0.3810 - val_auc: 0.7557\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.76645\n",
      "Epoch 33/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3429 - auc: 0.8129 - val_loss: 0.3873 - val_auc: 0.7472\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.76645\n",
      "Epoch 34/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3407 - auc: 0.8148 - val_loss: 0.3839 - val_auc: 0.7509\n",
      "\n",
      "Epoch 00034: val_auc did not improve from 0.76645\n",
      "Epoch 35/50\n",
      "61178/61178 [==============================] - 27s 445us/step - loss: 0.3366 - auc: 0.8212 - val_loss: 0.3909 - val_auc: 0.7475\n",
      "\n",
      "Epoch 00035: val_auc did not improve from 0.76645\n",
      "Epoch 36/50\n",
      "61178/61178 [==============================] - 27s 445us/step - loss: 0.3324 - auc: 0.8267 - val_loss: 0.3947 - val_auc: 0.7525\n",
      "\n",
      "Epoch 00036: val_auc did not improve from 0.76645\n",
      "Epoch 37/50\n",
      "61178/61178 [==============================] - 27s 446us/step - loss: 0.3311 - auc: 0.8304 - val_loss: 0.3887 - val_auc: 0.7454\n",
      "\n",
      "Epoch 00037: val_auc did not improve from 0.76645\n",
      "Epoch 38/50\n",
      "61178/61178 [==============================] - 27s 445us/step - loss: 0.3267 - auc: 0.8369 - val_loss: 0.4000 - val_auc: 0.7405\n",
      "\n",
      "Epoch 00038: val_auc did not improve from 0.76645\n",
      "Epoch 39/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3223 - auc: 0.8417 - val_loss: 0.3944 - val_auc: 0.7417\n",
      "\n",
      "Epoch 00039: val_auc did not improve from 0.76645\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3180 - auc: 0.8483 - val_loss: 0.4003 - val_auc: 0.7420\n",
      "\n",
      "Epoch 00040: val_auc did not improve from 0.76645\n",
      "Epoch 41/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.3150 - auc: 0.8518 - val_loss: 0.4068 - val_auc: 0.7370\n",
      "\n",
      "Epoch 00041: val_auc did not improve from 0.76645\n",
      "Epoch 42/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3108 - auc: 0.8564 - val_loss: 0.4056 - val_auc: 0.7371\n",
      "\n",
      "Epoch 00042: val_auc did not improve from 0.76645\n",
      "Epoch 43/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3053 - auc: 0.8634 - val_loss: 0.4161 - val_auc: 0.7252\n",
      "\n",
      "Epoch 00043: val_auc did not improve from 0.76645\n",
      "Epoch 44/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.3005 - auc: 0.8686 - val_loss: 0.4169 - val_auc: 0.7277\n",
      "\n",
      "Epoch 00044: val_auc did not improve from 0.76645\n",
      "Epoch 45/50\n",
      "61178/61178 [==============================] - 27s 444us/step - loss: 0.2967 - auc: 0.8737 - val_loss: 0.4218 - val_auc: 0.7333\n",
      "\n",
      "Epoch 00045: val_auc did not improve from 0.76645\n",
      "Epoch 46/50\n",
      "61178/61178 [==============================] - 27s 442us/step - loss: 0.2896 - auc: 0.8772 - val_loss: 0.4296 - val_auc: 0.7302\n",
      "\n",
      "Epoch 00046: val_auc did not improve from 0.76645\n",
      "Epoch 47/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.2869 - auc: 0.8810 - val_loss: 0.4310 - val_auc: 0.7332\n",
      "\n",
      "Epoch 00047: val_auc did not improve from 0.76645\n",
      "Epoch 48/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.2828 - auc: 0.8851 - val_loss: 0.4408 - val_auc: 0.7193\n",
      "\n",
      "Epoch 00048: val_auc did not improve from 0.76645\n",
      "Epoch 49/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.2758 - auc: 0.8912 - val_loss: 0.4414 - val_auc: 0.7262\n",
      "\n",
      "Epoch 00049: val_auc did not improve from 0.76645\n",
      "Epoch 50/50\n",
      "61178/61178 [==============================] - 27s 443us/step - loss: 0.2728 - auc: 0.8925 - val_loss: 0.4471 - val_auc: 0.7235\n",
      "\n",
      "Epoch 00050: val_auc did not improve from 0.76645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x245bb427ac8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting\n",
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filepath=\"weights_2.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,tensorboard]\n",
    "model.fit([X_train,X_train_school_state,X_train_project_grade,X_train_project_cat,X_train_project_subcat,\n",
    "           X_train_teacher_prefix,train['remaining_input']], y_train, nb_epoch=50,verbose=1,batch_size=256,\n",
    "          validation_data=([X_cv,X_cv_school_state,X_cv_project_grade,X_cv_project_cat,X_cv_project_subcat,\n",
    "           X_cv_teacher_prefix,cv['remaining_input']]  , y_cv),callbacks =callbacks_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model with best obtained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 1\n",
    "input1 = Input(shape=(250,))\n",
    "x1 = Embedding(input_dim=56913,output_dim= 300,weights=[embedding_mat(feature_names[5])],trainable=False)(input1)\n",
    "x1 = SpatialDropout1D(0.3)(x1)\n",
    "x1 = CuDNNLSTM(128,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "#input 2\n",
    "input2 = Input(shape=(1,))\n",
    "x2 = Embedding(input_dim= 52,output_dim= 2)(input2)\n",
    "#x2 = SpatialDropout1D(0.3)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "#input 3\n",
    "input3 = Input(shape=(1,))\n",
    "x3 = Embedding(input_dim= 5,output_dim= 2)(input3)\n",
    "#x3 = SpatialDropout1D(0.3)(x3)\n",
    "x3 = Flatten()(x3)\n",
    "\n",
    "#input 4\n",
    "input4 = Input(shape=(1,))\n",
    "x4 = Embedding(input_dim=50,output_dim= 2)(input4)\n",
    "#x4 = SpatialDropout1D(0.3)(x4)\n",
    "x4 = Flatten()(x4)\n",
    "\n",
    "#input 5\n",
    "input5 = Input(shape=(1,))\n",
    "x5 = Embedding(input_dim= 385,output_dim= 50)(input5)\n",
    "#x5 = SpatialDropout1D(0.3)(x5)\n",
    "x5 = Flatten()(x5)\n",
    "\n",
    "#input 6\n",
    "input6 = Input(shape=(1,))\n",
    "x6 = Embedding(input_dim= 6,output_dim= 5)(input6)\n",
    "#x6 = SpatialDropout1D(0.3)(x6)\n",
    "x6 = Flatten()(x6)\n",
    "\n",
    "#input 7\n",
    "input7 = Input(shape=(1,))\n",
    "x7 = Dense(16,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(input7)\n",
    "#x7 = Flatten()(x7)\n",
    "#merging all the inputs \n",
    "concat = concatenate([x1,x2,x3,x4,x5,x6,x7])\n",
    "#x = BatchNormalization()(concat)\n",
    "\n",
    "x = Dense(128,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    " \n",
    "# create model with seven inputs\n",
    "model = Model([input1,input2,input3,input4,input5,input6,input7], output)\n",
    "model.load_weights(\"weights_2.best.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0006,decay = 1e-4),metrics=[auc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc for test data: 0.766\n",
      "Auc for CV data: 0.766\n",
      "Auc for train data: 0.818\n"
     ]
    }
   ],
   "source": [
    "print(\"Auc for test data: %0.3f\"%roc_auc_score(y_test,model.predict([X_test,X_test_school_state,X_test_project_grade,X_test_project_cat,X_test_project_subcat,\n",
    "          X_test_teacher_prefix,test['remaining_input']])))\n",
    "print(\"Auc for CV data: %0.3f\"%roc_auc_score(y_cv,model.predict([X_cv,X_cv_school_state,X_cv_project_grade,X_cv_project_cat,X_cv_project_subcat,\n",
    "           X_cv_teacher_prefix,cv['remaining_input']])))\n",
    "print(\"Auc for train data: %0.3f\"%roc_auc_score(y_train,model.predict([X_train,X_train_school_state,X_train_project_grade,X_train_project_cat,X_train_project_subcat,\n",
    "           X_train_teacher_prefix,train['remaining_input']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'model_2_epoch_auc_loss.jpg'>\n",
    "<img src = 'model_2_epoch_val_auc_loss.jpg'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
