{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing all the necessary lib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer,one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, BatchNormalization,concatenate,Flatten,Embedding,Dense,Dropout,MaxPooling2D,Reshape,CuDNNLSTM,SpatialDropout1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import Model,Input\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.initializers import he_uniform,he_normal\n",
    "import keras.backend as k\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n",
    "from time import time\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from scipy.sparse import hstack\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>...</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>presence_of_the_numerical_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>p253737</td>\n",
       "      <td>c90749f5d961ff158d4b4d1e7dc665fc</td>\n",
       "      <td>mrs</td>\n",
       "      <td>in</td>\n",
       "      <td>2016-12-05 13:43:57</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl_literacy</td>\n",
       "      <td>educational support english learners home</td>\n",
       "      <td>...</td>\n",
       "      <td>\\\"The limits of your language are the limits o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need opportunities to practice beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>students english learners working english seco...</td>\n",
       "      <td>154.60</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>p258326</td>\n",
       "      <td>897464ce9ddc600bced1151f324dd63a</td>\n",
       "      <td>mr</td>\n",
       "      <td>fl</td>\n",
       "      <td>2016-10-25 09:22:10</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>history_civics_health_sports</td>\n",
       "      <td>civics_government_teamsports</td>\n",
       "      <td>wanted projector hungry learners</td>\n",
       "      <td>...</td>\n",
       "      <td>The projector we need for our school is very c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a projector to help with view...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>students arrive school eager learn polite gene...</td>\n",
       "      <td>299.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>p182444</td>\n",
       "      <td>3465aaf82da834c0582ebd0ef8040ca0</td>\n",
       "      <td>ms</td>\n",
       "      <td>az</td>\n",
       "      <td>2016-08-31 12:03:56</td>\n",
       "      <td>grades_6_8</td>\n",
       "      <td>health_sports</td>\n",
       "      <td>health_wellness_teamsports</td>\n",
       "      <td>soccer equipment awesome middle school students</td>\n",
       "      <td>...</td>\n",
       "      <td>The students on the campus come to school know...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need shine guards, athletic socks,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>true champions not always ones win guts mia ha...</td>\n",
       "      <td>516.85</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                        teacher_id teacher_prefix  \\\n",
       "0           0  p253737  c90749f5d961ff158d4b4d1e7dc665fc            mrs   \n",
       "1           1  p258326  897464ce9ddc600bced1151f324dd63a             mr   \n",
       "2           2  p182444  3465aaf82da834c0582ebd0ef8040ca0             ms   \n",
       "\n",
       "  school_state project_submitted_datetime project_grade_category  \\\n",
       "0           in        2016-12-05 13:43:57          grades_prek_2   \n",
       "1           fl        2016-10-25 09:22:10             grades_6_8   \n",
       "2           az        2016-08-31 12:03:56             grades_6_8   \n",
       "\n",
       "     project_subject_categories project_subject_subcategories  \\\n",
       "0             literacy_language                  esl_literacy   \n",
       "1  history_civics_health_sports  civics_government_teamsports   \n",
       "2                 health_sports    health_wellness_teamsports   \n",
       "\n",
       "                                     project_title  \\\n",
       "0        educational support english learners home   \n",
       "1                 wanted projector hungry learners   \n",
       "2  soccer equipment awesome middle school students   \n",
       "\n",
       "                 ...                 \\\n",
       "0                ...                  \n",
       "1                ...                  \n",
       "2                ...                  \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  \\\"The limits of your language are the limits o...             NaN   \n",
       "1  The projector we need for our school is very c...             NaN   \n",
       "2  The students on the campus come to school know...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need opportunities to practice beg...   \n",
       "1             NaN  My students need a projector to help with view...   \n",
       "2             NaN  My students need shine guards, athletic socks,...   \n",
       "\n",
       "  teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            0                    0   \n",
       "1                                            7                    1   \n",
       "2                                            1                    0   \n",
       "\n",
       "                                               essay   price  quantity  \\\n",
       "0  students english learners working english seco...  154.60        23   \n",
       "1  students arrive school eager learn polite gene...  299.00         1   \n",
       "2  true champions not always ones win guts mia ha...  516.85        22   \n",
       "\n",
       "   presence_of_the_numerical_digits  \n",
       "0                                 0  \n",
       "1                                 0  \n",
       "2                                 0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "project_data = pd.read_csv('processed_train_data.csv')\n",
    "project_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding feature teacher number of previously posted project, presence of numerical digits, price and quantity\n",
    "project_data.drop(['Unnamed: 0'], axis =1 , inplace = True)\n",
    "class_label = project_data['project_is_approved']\n",
    "project_data['remaining_input'] = project_data['teacher_number_of_previously_posted_projects']  +\\\n",
    "                                    project_data['presence_of_the_numerical_digits']  + \\\n",
    "                                    project_data['price'] + project_data['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['total_txt'] = project_data['project_title'] + ' ' + project_data['essay'] + ' ' + project_data['project_resource_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.replace(to_replace=np.NaN, value= str('nan'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
       "       'project_submitted_datetime', 'project_grade_category',\n",
       "       'project_subject_categories', 'project_subject_subcategories',\n",
       "       'project_title', 'project_essay_1', 'project_essay_2',\n",
       "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
       "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
       "       'essay', 'price', 'quantity', 'presence_of_the_numerical_digits',\n",
       "       'remaining_input', 'total_txt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = project_data.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['id','teacher_id','project_submitted_datetime','project_title','project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4','project_resource_summary',\n",
    "       'teacher_number_of_previously_posted_projects', 'project_is_approved','price', 'quantity',\n",
    "        'presence_of_the_numerical_digits','essay']\n",
    "\n",
    "project_data.drop(labels=col,axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = project_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['teacher_prefix', 'school_state', 'project_grade_category',\n",
    "       'project_subject_categories', 'project_subject_subcategories','total_txt',\n",
    "       'remaining_input']\n",
    "project_data = project_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ranking(dataframe):\n",
    "    col_names = dataframe.columns\n",
    "    features = []\n",
    "    #performing train test split\n",
    "    train,test,y_train,y_test = train_test_split(dataframe, class_label , stratify = class_label, train_size = 0.7)\n",
    "\n",
    "    train,cv,y_train,y_cv = train_test_split(train,y_train,stratify = y_train,train_size = 0.8)\n",
    "    for col in col_names[5:6]:\n",
    "        print(col)\n",
    "        bag_of_words = CountVectorizer(lowercase= False)\n",
    "        bow_words = bag_of_words.fit_transform(train[col])\n",
    "        print(bow_words.shape)\n",
    "        \n",
    "        #Lets now store the document term matrix in a dictionary.\n",
    "        freqs = bow_words.sum(axis=0).A1\n",
    "        index = freqs.argsort()\n",
    "        words = bag_of_words.get_feature_names()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Assigning Rank to each word based on its freq of occurance. Word with highest freq is assigned rank 1 \n",
    "        word_rank = dict()\n",
    "        rank = 1\n",
    "        for i in index[::-1]:\n",
    "            k = words[i]\n",
    "            word_rank[k] = rank\n",
    "            rank+=1\n",
    "        features.append(word_rank)\n",
    "\n",
    "        #Every word in each review is replaced by its rank\n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in train[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        train[col] = rank\n",
    "        \n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in test[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        test[col] = rank\n",
    "        \n",
    "        rank = [] # list of all the review with words replaced with rank\n",
    "        for sent in cv[col].values:\n",
    "            txt_row = []\n",
    "            for word in sent.split():\n",
    "                if word in word_rank.keys():\n",
    "                    txt_row.append(word_rank[word])\n",
    "                else:\n",
    "                    pass\n",
    "            rank.append(txt_row)\n",
    "        \n",
    "        cv[col] = rank\n",
    "    return train,test,cv,y_train,y_test,y_cv,features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_txt\n",
      "(61178, 56850)\n"
     ]
    }
   ],
   "source": [
    "train,test,cv,y_train,y_test,y_cv,feature_names = word_ranking(project_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Train dataset:  61178\n",
      "Shape of the Test dataset:  32775\n",
      "Shape of the CV dataset:  15295\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Train dataset: \", train.shape[0])\n",
    "print(\"Shape of the Test dataset: \", test.shape[0])\n",
    "print(\"Shape of the CV dataset: \", cv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61178,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting class labels to categorical variables\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_cv = to_categorical(y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the Text part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 250)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  1351    14   427   834   243  2777\n",
      "  2080    35     1   945     1     2   879  2219    46     2  1929   289\n",
      "  2219   970  1929  1980     2  6190    29  1050  1450   434  1964  1079\n",
      "    28    71    28     2  1839  2556  6054  5449     2    28     1  9466\n",
      "  4313  2199  3114  6054  6054  4047  3108  6054    73  2757    14  2556\n",
      "     2   959  5655   895     1   269    66  1618     1   328   372    52\n",
      "    14  3051   383  1050    86   168   243     1   174   539    91  2556\n",
      "  6054   962  1448     1    78   205   252   380  1351  1715 55189 51358\n",
      "  1749   700    39     1  1507    14   128  3940  1031  6054  1351    41\n",
      "     1   230    63    14  7365  1554    21    24    29  9990    14  2556\n",
      "     1    27   160     2   232   123   341   245   282  1155     2     8\n",
      "   247   477  1931    83  1387     1   645    46     2   375    28  6054\n",
      "   712 13280   427   834    13    10     1     3   241   316    78    70\n",
      "  2463   251   632   700     9  2794    76    45 38324 34918  2860   298\n",
      "   833     4    15   298  2498     9  2794  4076     1   671]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_review_length = 250\n",
    "X_train = pad_sequences(train['total_txt'], maxlen=max_review_length)  #padding zeros at the begining of each review to make max len as 200\n",
    "X_test = pad_sequences(test['total_txt'], maxlen=max_review_length)\n",
    "X_cv = pad_sequences(cv['total_txt'], maxlen=max_review_length)\n",
    "print(X_train.shape)\n",
    "print(X_train[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the school state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 51)\n"
     ]
    }
   ],
   "source": [
    "token_sc_stat = CountVectorizer()\n",
    "\n",
    "# integer encode the documents\n",
    "school_state_train = token_sc_stat.fit_transform(train['school_state'])\n",
    "school_state_test = token_sc_stat.transform(test['school_state'])\n",
    "school_state_cv = token_sc_stat.transform(cv['school_state'])\n",
    "\n",
    "print(school_state_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project_grade_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 4)\n"
     ]
    }
   ],
   "source": [
    "token_grd_cat = CountVectorizer()\n",
    "\n",
    "# integer encode the documents\n",
    "project_grade_train = token_grd_cat.fit_transform(train['project_grade_category'])\n",
    "project_grade_test = token_grd_cat.transform(test['project_grade_category'])\n",
    "project_grade_cv = token_grd_cat.transform(cv['project_grade_category'])\n",
    "\n",
    "print(project_grade_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 51)\n"
     ]
    }
   ],
   "source": [
    "token_prj_cat = CountVectorizer()\n",
    "\n",
    "# integer encode the documents\n",
    "project_cat_train = token_prj_cat.fit_transform(train['project_subject_categories'])\n",
    "project_cat_test = token_prj_cat.transform(test['project_subject_categories'])\n",
    "project_cat_cv = token_prj_cat.transform(cv['project_subject_categories'])\n",
    "\n",
    "\n",
    "print(project_cat_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the project subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 384)\n"
     ]
    }
   ],
   "source": [
    "token_sub_cat = CountVectorizer()\n",
    "\n",
    "# integer encode the documents\n",
    "project_subcat_train = token_sub_cat.fit_transform(train['project_subject_subcategories'])\n",
    "project_subcat_test = token_sub_cat.transform(test['project_subject_subcategories'])\n",
    "project_subcat_cv = token_sub_cat.transform(cv['project_subject_subcategories'])\n",
    "\n",
    "print(project_subcat_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the teacher prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61178, 5)\n"
     ]
    }
   ],
   "source": [
    "token_pre = CountVectorizer()\n",
    "\n",
    "# integer encode the documents\n",
    "teacher_prefix_train = token_pre.fit_transform(train['teacher_prefix'])\n",
    "teacher_prefix_test = token_pre.transform(test['teacher_prefix'])\n",
    "teacher_prefix_cv = token_pre.transform(cv['teacher_prefix'])\n",
    "print(teacher_prefix_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2_train = hstack((school_state_train,project_grade_train,project_cat_train,project_subcat_train,teacher_prefix_train,train['remaining_input'][:,None]))\n",
    "\n",
    "input2_cv = hstack((school_state_cv,project_grade_cv,project_cat_cv,project_subcat_cv,teacher_prefix_cv,cv['remaining_input'][:,None]))\n",
    "input2_test = hstack((school_state_test,project_grade_test,project_cat_test,project_subcat_test,teacher_prefix_test,test['remaining_input'][:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61178, 496)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=input2_train.todense()\n",
    "test = input2_test.todense()\n",
    "cv = input2_cv.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.resize(train,new_shape=(61178,495,1))\n",
    "test =np.resize(test,new_shape=(32775,495,1))\n",
    "cv = np.resize(cv,new_shape=(15295,495,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61178, 495, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    " ### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('glove_vectors.pickle', 'rb')      \n",
    "db = pickle.load(dbfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting word vectors with 50 dim\n",
    "def embedding_mat(word_index,embedding_dim = 300):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = db.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC score \n",
    "def auc( y_true, y_pred ) :\n",
    "    score = tf.py_func( lambda y_true, y_pred : roc_auc_score( y_true, y_pred).astype('float32'),\n",
    "                        [y_true, y_pred],\n",
    "                        'float32',\n",
    "                        stateful=True,\n",
    "                        name='sklearnAUC' )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-28-f369317cd349>:7: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 250, 300)     17055300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 495, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 493, 64)      256         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 250, 256)     571392      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 491, 64)      12352       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64000)        0           cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 31424)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 95424)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          28627500    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          77056       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 46,378,034\n",
      "Trainable params: 29,322,222\n",
      "Non-trainable params: 17,055,812\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input 1\n",
    "input1 = Input(batch_shape=(None,250))\n",
    "x1 = Embedding(input_dim=56851,output_dim= 300,weights=[embedding_mat(feature_names[0])],trainable = False)(input1)\n",
    "x1 = SpatialDropout1D(0.3)(x1)\n",
    "x1 = CuDNNLSTM(256,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "# input 2\n",
    "input2 = Input(shape=(495,1))\n",
    "x2 = Conv1D(filters=64,kernel_size=3,strides=1)(input2)\n",
    "x2 = Conv1D(filters=64,kernel_size=3,strides=1)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "# merging both the inputs\n",
    "concat = concatenate([x1,x2])\n",
    "x = Dense(300,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(128,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    " \n",
    "# create model with two inputs\n",
    "model = Model([input1,input2], output)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.adam(lr=0.0006,decay = 1e-4), metrics=[auc])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 61178 samples, validate on 15295 samples\n",
      "Epoch 1/25\n",
      "61178/61178 [==============================] - 73s 1ms/step - loss: 0.7561 - auc: 0.5113 - val_loss: 0.6153 - val_auc: 0.4866\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.48661, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 2/25\n",
      "61178/61178 [==============================] - 68s 1ms/step - loss: 0.6240 - auc: 0.5085 - val_loss: 0.5780 - val_auc: 0.5093\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.48661 to 0.50927, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 3/25\n",
      "61178/61178 [==============================] - 68s 1ms/step - loss: 0.5712 - auc: 0.5117 - val_loss: 0.5690 - val_auc: 0.5372\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.50927 to 0.53719, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 4/25\n",
      "61178/61178 [==============================] - 68s 1ms/step - loss: 0.5682 - auc: 0.5144 - val_loss: 0.5757 - val_auc: 0.5250\n",
      "\n",
      "Epoch 00004: val_auc did not improve from 0.53719\n",
      "Epoch 5/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.5548 - auc: 0.5266 - val_loss: 0.5276 - val_auc: 0.6427\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.53719 to 0.64274, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 6/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.5126 - auc: 0.6593 - val_loss: 0.4907 - val_auc: 0.7158\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.64274 to 0.71578, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 7/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.5045 - auc: 0.6689 - val_loss: 0.4872 - val_auc: 0.7216\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.71578 to 0.72158, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 8/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4838 - auc: 0.7081 - val_loss: 0.4841 - val_auc: 0.7335\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.72158 to 0.73353, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 9/25\n",
      "61178/61178 [==============================] - 72s 1ms/step - loss: 0.4731 - auc: 0.7227 - val_loss: 0.4649 - val_auc: 0.7352\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.73353 to 0.73522, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 10/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4621 - auc: 0.7328 - val_loss: 0.4526 - val_auc: 0.7484\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.73522 to 0.74840, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 11/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4552 - auc: 0.7435 - val_loss: 0.4550 - val_auc: 0.7501\n",
      "\n",
      "Epoch 00011: val_auc improved from 0.74840 to 0.75005, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 12/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4484 - auc: 0.7521 - val_loss: 0.4458 - val_auc: 0.7589\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.75005 to 0.75889, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 13/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4430 - auc: 0.7528 - val_loss: 0.4434 - val_auc: 0.7561\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.75889\n",
      "Epoch 14/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.4352 - auc: 0.7677 - val_loss: 0.4358 - val_auc: 0.7653\n",
      "\n",
      "Epoch 00014: val_auc improved from 0.75889 to 0.76528, saving model to weights_3.best_copy.hdf5\n",
      "Epoch 15/25\n",
      "61178/61178 [==============================] - 69s 1ms/step - loss: 0.4296 - auc: 0.7705 - val_loss: 0.4359 - val_auc: 0.7607\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.76528\n",
      "Epoch 16/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.4212 - auc: 0.7778 - val_loss: 0.4339 - val_auc: 0.7578\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.76528\n",
      "Epoch 17/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.4168 - auc: 0.7822 - val_loss: 0.4331 - val_auc: 0.7573\n",
      "\n",
      "Epoch 00017: val_auc did not improve from 0.76528\n",
      "Epoch 18/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.4081 - auc: 0.7883 - val_loss: 0.4225 - val_auc: 0.7609\n",
      "\n",
      "Epoch 00018: val_auc did not improve from 0.76528\n",
      "Epoch 19/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.4012 - auc: 0.7965 - val_loss: 0.4339 - val_auc: 0.7576\n",
      "\n",
      "Epoch 00019: val_auc did not improve from 0.76528\n",
      "Epoch 20/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3947 - auc: 0.8064 - val_loss: 0.4283 - val_auc: 0.7542\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.76528\n",
      "Epoch 21/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3888 - auc: 0.8103 - val_loss: 0.4225 - val_auc: 0.7551\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.76528\n",
      "Epoch 22/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3852 - auc: 0.8176 - val_loss: 0.4258 - val_auc: 0.7568\n",
      "\n",
      "Epoch 00022: val_auc did not improve from 0.76528\n",
      "Epoch 23/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3788 - auc: 0.8261 - val_loss: 0.4291 - val_auc: 0.7488\n",
      "\n",
      "Epoch 00023: val_auc did not improve from 0.76528\n",
      "Epoch 24/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3729 - auc: 0.8318 - val_loss: 0.4409 - val_auc: 0.7489\n",
      "\n",
      "Epoch 00024: val_auc did not improve from 0.76528\n",
      "Epoch 25/25\n",
      "61178/61178 [==============================] - 70s 1ms/step - loss: 0.3694 - auc: 0.8397 - val_loss: 0.4351 - val_auc: 0.7409\n",
      "\n",
      "Epoch 00025: val_auc did not improve from 0.76528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a058bf278>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting\n",
    "filepath=\"weights_3.best_copy.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,tensorboard]\n",
    "#my_callbacks = [EarlyStopping(monitor='auc', patience=300, verbose=1, mode='max')]\n",
    "model.fit([X_train,train], y_train, nb_epoch=25,verbose=1,batch_size=256,validation_data=([X_cv,\n",
    "        cv], y_cv),callbacks = callbacks_list,class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model with best parameters : i.e the best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 1\n",
    "input1 = Input(batch_shape=(None,250))\n",
    "x1 = Embedding(input_dim=56851,output_dim= 300,weights=[embedding_mat(feature_names[0])],trainable = False)(input1)\n",
    "x1 = SpatialDropout1D(0.3)(x1)\n",
    "x1 = CuDNNLSTM(256,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "# input 2\n",
    "input2 = Input(shape=(495,1))\n",
    "x2 = Conv1D(filters=64,kernel_size=3,strides=1)(input2)\n",
    "x2 = Conv1D(filters=64,kernel_size=3,strides=1)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "# merging both the inputs\n",
    "concat = concatenate([x1,x2])\n",
    "x = Dense(300,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(128,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    " \n",
    "# create model with two inputs\n",
    "model = Model([input1,input2], output)\n",
    "model.load_weights(\"weights_3.best_copy.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.adam(lr=0.0006,decay = 1e-4), metrics=[auc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc for test data: 0.757\n",
      "Auc for CV data: 0.765\n",
      "Auc for train data: 0.799\n"
     ]
    }
   ],
   "source": [
    "print(\"Auc for test data: %0.3f\"%roc_auc_score(y_test,model.predict([X_test,test])))\n",
    "print(\"Auc for CV data: %0.3f\"%roc_auc_score(y_cv,model.predict([X_cv,cv])))\n",
    "print(\"Auc for train data: %0.3f\"%roc_auc_score(y_train,model.predict([X_train,train])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'model_3_epoch_auc_loss.jpg'>\n",
    "<img src = 'model_3_epoch_val_auc_loss.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
